{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEMamba(\n",
       "  (dense_encoder): DenseEncoder(\n",
       "    (dense_conv_1): Sequential(\n",
       "      (0): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): PReLU(num_parameters=64)\n",
       "    )\n",
       "    (dense_block): DenseBlock(\n",
       "      (dense_block): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dense_conv_2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 2))\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): PReLU(num_parameters=64)\n",
       "    )\n",
       "  )\n",
       "  (TSMamba): ModuleList(\n",
       "    (0-3): 4 x TFMambaBlock(\n",
       "      (time_mamba): MambaBlock(\n",
       "        (forward_blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (mixer): Mamba(\n",
       "              (in_proj): Linear(in_features=64, out_features=512, bias=False)\n",
       "              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "              (act): SiLU()\n",
       "              (x_proj): Linear(in_features=256, out_features=36, bias=False)\n",
       "              (dt_proj): Linear(in_features=4, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "            )\n",
       "            (norm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (backward_blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (mixer): Mamba(\n",
       "              (in_proj): Linear(in_features=64, out_features=512, bias=False)\n",
       "              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "              (act): SiLU()\n",
       "              (x_proj): Linear(in_features=256, out_features=36, bias=False)\n",
       "              (dt_proj): Linear(in_features=4, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "            )\n",
       "            (norm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (freq_mamba): MambaBlock(\n",
       "        (forward_blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (mixer): Mamba(\n",
       "              (in_proj): Linear(in_features=64, out_features=512, bias=False)\n",
       "              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "              (act): SiLU()\n",
       "              (x_proj): Linear(in_features=256, out_features=36, bias=False)\n",
       "              (dt_proj): Linear(in_features=4, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "            )\n",
       "            (norm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (backward_blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (mixer): Mamba(\n",
       "              (in_proj): Linear(in_features=64, out_features=512, bias=False)\n",
       "              (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "              (act): SiLU()\n",
       "              (x_proj): Linear(in_features=256, out_features=36, bias=False)\n",
       "              (dt_proj): Linear(in_features=4, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=64, bias=False)\n",
       "            )\n",
       "            (norm): RMSNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (tlinear): ConvTranspose1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "      (flinear): ConvTranspose1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (mask_decoder): MagDecoder(\n",
       "    (dense_block): DenseBlock(\n",
       "      (dense_block): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mask_conv): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(1, 3), stride=(1, 2))\n",
       "      (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (3): PReLU(num_parameters=1)\n",
       "      (4): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (lsigmoid): LearnableSigmoid2D()\n",
       "  )\n",
       "  (phase_decoder): PhaseDecoder(\n",
       "    (dense_block): DenseBlock(\n",
       "      (dense_block): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
       "          (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (2): PReLU(num_parameters=64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (phase_conv): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(1, 3), stride=(1, 2))\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): PReLU(num_parameters=64)\n",
       "    )\n",
       "    (phase_conv_r): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (phase_conv_i): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "from models.generator import SEMamba\n",
    "import torch\n",
    "from utils.util import load_config\n",
    "import os\n",
    "import librosa\n",
    "from models.stfts import mag_phase_stft, mag_phase_istft\n",
    "from models.pcs400 import cal_pcs\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "# config = '/disk4/chocho/SEMamba/exp/VCTK/dep3_h32_tf4_ds32_dc3_ex4/config.yaml'\n",
    "# checkpoint_file = '/disk4/chocho/SEMamba/exp/VCTK/dep3_h32_tf4_ds32_dc3_ex4/g_00025000.pth'\n",
    "config = '/disk4/chocho/SEMamba/exp/VCTK-400/dep4_h64_tf4_ds16_dc4_ex4/config.yaml'\n",
    "checkpoint_file = \"/disk4/chocho/SEMamba/exp/VCTK-400/dep4_h64_tf4_ds16_dc4_ex4/g_00093000.pth\"\n",
    "cfg = load_config(config)\n",
    "n_fft, hop_size, win_size = cfg['stft_cfg']['n_fft'], cfg['stft_cfg']['hop_size'], cfg['stft_cfg']['win_size']\n",
    "compress_factor = cfg['model_cfg']['compress_factor']\n",
    "sampling_rate = cfg['stft_cfg']['sampling_rate']\n",
    "model = SEMamba(cfg).to(device)\n",
    "state_dict = torch.load(checkpoint_file, map_location=device)\n",
    "model.load_state_dict(state_dict['generator'])\n",
    "model.eval()\n",
    "# output_folder = '/disk4/chocho/SEMamba/_202505/encoder-mamba-decoder'\n",
    "# os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoder = model.dense_encoder\n",
    "mask_decoder = model.mask_decoder\n",
    "phase_decoder = model.phase_decoder\n",
    "TSMamba = model.TSMamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseEncoder(\n",
       "  (dense_conv_1): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): PReLU(num_parameters=64)\n",
       "  )\n",
       "  (dense_block): DenseBlock(\n",
       "    (dense_block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense_conv_2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 2))\n",
       "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): PReLU(num_parameters=64)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "feature_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # x1 = x.clone()\n",
    "                # b, c, t, f = x1.size()\n",
    "                # x1 = x1.permute(0, 3, 2, 1).contiguous().view(b*f, t, c)\n",
    "                # x1 = block.tlinear( block.time_mamba(x1).permute(0,2,1) ).permute(0,2,1) + x1\n",
    "                # x1 = x1.view(b, f, t, c).permute(0, 2, 1, 3).contiguous().view(b*t, f, c)\n",
    "                # x1 = block.flinear( block.freq_mamba(x1).permute(0,2,1) ).permute(0,2,1) + x1\n",
    "                # x1 = x1.view(b, t, f, c).permute(0, 3, 1, 2)\n",
    "                \n",
    "                # x = block(x)\n",
    "                # pps(x)\n",
    "                # pp(id(x),id(x1))\n",
    "                # pp((x != x1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk4/chocho/SEMamba/_test_feature_map_noisy p226_018.wav\n",
      "noisy_wav's shape: (94035,)\n",
      "norm_factor: 20.062986373901367\n",
      "noisy_wav's shape: torch.Size([1, 94035])\n",
      "noisy_mag's shape: torch.Size([1, 1, 941, 257])\n",
      "noisy_pha's shape: torch.Size([1, 1, 941, 257])\n",
      "x's shape: torch.Size([1, 2, 941, 257])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "noisy_mag's shape: torch.Size([1, 1, 941, 257])\n",
      "mask_decoder(x)'s shape: torch.Size([1, 1, 941, 257])\n",
      "phase_decoder(x)'s shape: torch.Size([1, 1, 941, 257])\n",
      "denoised_mag's shape: torch.Size([1, 257, 941])\n",
      "denoised_pha's shape: torch.Size([1, 257, 941])\n",
      "audio_g's shape: torch.Size([1, 94000])\n",
      "audio_g's shape: (94000,)\n"
     ]
    }
   ],
   "source": [
    "from c66 import pp, pps\n",
    "from einops import rearrange\n",
    "with torch.no_grad():\n",
    "    # for clean_or_noisy in  [\"clean\",\"noisy\"]:\n",
    "    for clean_or_noisy in  [\"noisy\"]:\n",
    "        input_folder = f'/disk4/chocho/SEMamba/_test_feature_map_{clean_or_noisy}'\n",
    "        for i, fname in enumerate(os.listdir( input_folder )):\n",
    "            print(input_folder, fname)\n",
    "            noisy_wav, _ = librosa.load(os.path.join( input_folder, fname ), sr=sampling_rate)\n",
    "            pps(noisy_wav)\n",
    "            noisy_wav = torch.FloatTensor(noisy_wav).to(device)\n",
    "            \n",
    "            norm_factor = torch.sqrt(len(noisy_wav) / torch.sum(noisy_wav ** 2.0)).to(device)\n",
    "            pp(norm_factor)\n",
    "            noisy_wav = (noisy_wav * norm_factor).unsqueeze(0)\n",
    "            pps(noisy_wav)\n",
    "            noisy_mag, noisy_pha, noisy_com = mag_phase_stft(noisy_wav, n_fft, hop_size, win_size, compress_factor)\n",
    "            \n",
    "            # feature_encoder\n",
    "            # Reshape inputs\n",
    "            noisy_mag = rearrange(noisy_mag, 'b f t -> b t f').unsqueeze(1)  # [B F T] -> [B, 1, T, F]\n",
    "            noisy_pha = rearrange(noisy_pha, 'b f t -> b t f').unsqueeze(1)  # [B F T] -> [B, 1, T, F]\n",
    "            pps(noisy_mag, noisy_pha)\n",
    "\n",
    "            # Concatenate magnitude and phase inputs\n",
    "            x = torch.cat((noisy_mag, noisy_pha), dim=1)  # [B, 2, T, F]\n",
    "            pps(x)\n",
    "\n",
    "            # Feature Encoder\n",
    "            x = feature_encoder(x)\n",
    "            pps(x)\n",
    "            \n",
    "            # TF-Mamba\n",
    "            # TSMamba is a instance of TFMambaBlock\n",
    "            for block in TSMamba:\n",
    "                b, c, t, f = x.size()\n",
    "                x = x.permute(0, 3, 2, 1).contiguous().view(b*f, t, c)\n",
    "                print(\"---\")\n",
    "                pps(x)\n",
    "                x = block.tlinear( block.time_mamba(x).permute(0,2,1) ).permute(0,2,1) + x\n",
    "                pps(x)\n",
    "                x = x.view(b, f, t, c).permute(0, 2, 1, 3).contiguous().view(b*t, f, c)\n",
    "                x = block.flinear( block.freq_mamba(x).permute(0,2,1) ).permute(0,2,1) + x\n",
    "                pps(x)\n",
    "                x = x.view(b, t, f, c).permute(0, 3, 1, 2)\n",
    "                pps(x)\n",
    "                print(\"---\")\n",
    "\n",
    "            # Mag, Pha Decoder\n",
    "            denoised_mag = rearrange(mask_decoder(x) * noisy_mag, 'b c t f -> b f t c').squeeze(-1)\n",
    "            denoised_pha = rearrange(phase_decoder(x), 'b c t f -> b f t c').squeeze(-1)\n",
    "            pps(noisy_mag)\n",
    "            pps(mask_decoder(x), phase_decoder(x))\n",
    "            pps(denoised_mag, denoised_pha)\n",
    "            \n",
    "            # \n",
    "            audio_g = mag_phase_istft(denoised_mag, denoised_pha, n_fft, hop_size, win_size, compress_factor)\n",
    "            pps(audio_g)\n",
    "            audio_g = audio_g / norm_factor\n",
    "            audio_g = cal_pcs(audio_g.squeeze().cpu().numpy())\n",
    "            pps(audio_g)\n",
    "            \n",
    "            break\n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk4/chocho/SEMamba/_test_feature_map_noisy p226_018.wav\n",
      "norm_factor: 20.062986373901367\n",
      "noisy_wav's shape: torch.Size([1, 94035])\n",
      "x's shape: torch.Size([1, 2, 941, 257])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "denoised_mag's shape: torch.Size([1, 257, 941])\n",
      "denoised_pha's shape: torch.Size([1, 257, 941])\n",
      "audio_g's shape: (94000,)\n"
     ]
    }
   ],
   "source": [
    "from c66 import pp, pps\n",
    "from einops import rearrange\n",
    "with torch.no_grad():\n",
    "    # for clean_or_noisy in  [\"clean\",\"noisy\"]:\n",
    "    for clean_or_noisy in  [\"noisy\"]:\n",
    "        input_folder = f'/disk4/chocho/SEMamba/_test_feature_map_{clean_or_noisy}'\n",
    "        for i, fname in enumerate(os.listdir( input_folder )):\n",
    "            print(input_folder, fname)\n",
    "            noisy_wav, _ = librosa.load(os.path.join( input_folder, fname ), sr=sampling_rate)\n",
    "            noisy_wav = torch.FloatTensor(noisy_wav).to(device)\n",
    "            \n",
    "            norm_factor = torch.sqrt(len(noisy_wav) / torch.sum(noisy_wav ** 2.0)).to(device)\n",
    "            # noisy_wav: [len_wav,] = [L,]\n",
    "            # norm_factor: [1,]\n",
    "            # !!!\n",
    "            \n",
    "            noisy_wav = (noisy_wav * norm_factor).unsqueeze(0)\n",
    "            # noisy_wav: [1, len_wav] = [1, L]\n",
    "            # !!!\n",
    "\n",
    "            noisy_mag, noisy_pha, noisy_com = mag_phase_stft(noisy_wav, n_fft, hop_size, win_size, compress_factor)\n",
    "            # noisy_mag, noisy_pha: [1, F, T] = [1, n_fft//2, len_wav//hop_size]\n",
    "            # !!!\n",
    "            \n",
    "            # feature_encoder\n",
    "            # Reshape inputs\n",
    "            noisy_mag = rearrange(noisy_mag, 'b f t -> b t f').unsqueeze(1)  # [B F T] -> [B, 1, T, F]\n",
    "            noisy_pha = rearrange(noisy_pha, 'b f t -> b t f').unsqueeze(1)  # [B F T] -> [B, 1, T, F]\n",
    "\n",
    "            # Concatenate magnitude and phase inputs\n",
    "            x = torch.cat((noisy_mag, noisy_pha), dim=1)  # [B, 2, T, F]\n",
    "\n",
    "            # Feature Encoder\n",
    "            x = feature_encoder(x)\n",
    "            # [B, 2, T, F] -> [B, h, T, F//2]\n",
    "            # !!!\n",
    "            \n",
    "            # TF-Mamba\n",
    "            # TSMamba is a instance of TFMambaBlock\n",
    "            for block in TSMamba:\n",
    "                b, c, t, f = x.size()\n",
    "                # b, c, t, f = [1, h, T, F]\n",
    "                \n",
    "                x = x.permute(0, 3, 2, 1).contiguous().view(b*f, t, c)\n",
    "                \n",
    "                x = block.tlinear( block.time_mamba(x).permute(0,2,1) ).permute(0,2,1) + x\n",
    "                # [F, T, h] -> [F, T, h]\n",
    "                # !!!\n",
    "                \n",
    "                x = x.view(b, f, t, c).permute(0, 2, 1, 3).contiguous().view(b*t, f, c)\n",
    "                \n",
    "                x = block.flinear( block.freq_mamba(x).permute(0,2,1) ).permute(0,2,1) + x\n",
    "                # [T, F, h] -> [T, F, h]\n",
    "                # !!!\n",
    "                \n",
    "                x = x.view(b, t, f, c).permute(0, 3, 1, 2)\n",
    "\n",
    "            # Mag, Pha Decoder\n",
    "            denoised_mag = rearrange(mask_decoder(x) * noisy_mag, 'b c t f -> b f t c').squeeze(-1)\n",
    "            # [1, 1, T, F] * [1, 1, T, F] = [1, 1, T, F] -> [1, F, T, 1] -> [1, F, T]\n",
    "            # !!!\n",
    "            \n",
    "            denoised_pha = rearrange(phase_decoder(x), 'b c t f -> b f t c').squeeze(-1)\n",
    "            # [1, 1, T, F] -> [1, F, T, 1] -> [1, F, T]\n",
    "            # !!!\n",
    "            \n",
    "            audio_g = mag_phase_istft(denoised_mag, denoised_pha, n_fft, hop_size, win_size, compress_factor)\n",
    "            # [1, ~L]\n",
    "            # !!!\n",
    "            \n",
    "            audio_g = audio_g / norm_factor\n",
    "            # !!!\n",
    "            \n",
    "            audio_g = cal_pcs(audio_g.squeeze().cpu().numpy())\n",
    "            # [~L,]\n",
    "            # !!!\n",
    "            \n",
    "            pps(audio_g)\n",
    "            \n",
    "            break\n",
    "            \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b f t -> b t f\".\n Input tensor shape: torch.Size([1, 1, 941, 257]). Additional info: {}.\n Wrong shape: expected 3 dims. Received 4-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEinopsError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/einops/einops.py:531\u001b[39m, in \u001b[36mreduce\u001b[39m\u001b[34m(tensor, pattern, reduction, **axes_lengths)\u001b[39m\n\u001b[32m    530\u001b[39m shape = backend.shape(tensor)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m recipe = \u001b[43m_prepare_transformation_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_names\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_recipe(\n\u001b[32m    533\u001b[39m     backend, recipe, cast(Tensor, tensor), reduction_type=reduction, axes_lengths=hashable_axes_lengths\n\u001b[32m    534\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/einops/einops.py:366\u001b[39m, in \u001b[36m_prepare_transformation_recipe\u001b[39m\u001b[34m(pattern, operation, axes_names, ndim)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ndim != \u001b[38;5;28mlen\u001b[39m(left.composition):\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWrong shape: expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(left.composition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dims. Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-dim tensor.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    367\u001b[39m left_composition = left.composition\n",
      "\u001b[31mEinopsError\u001b[39m: Wrong shape: expected 3 dims. Received 4-dim tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mEinopsError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcalflops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_flops\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# 使用 calflops 計算 FLOPs，將 args 改為列表\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         flops, macs, params = \u001b[43mcalculate_flops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoisy_mag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoisy_pha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 使用列表而非元組\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprint_results\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 顯示逐層結果\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m         \u001b[38;5;66;03m# print(f\"Total FLOPs for {fname}: {flops / 1e9:.3f} GFLOPs\")\u001b[39;00m\n\u001b[32m     10\u001b[39m         \u001b[38;5;66;03m# print(f\"Total Params: {params / 1e6:.3f} M\")\u001b[39;00m\n\u001b[32m     11\u001b[39m         \u001b[38;5;66;03m# print(f\"Total MACs: {macs / 1e9:.3f} GMACs\")\u001b[39;00m\n\u001b[32m     12\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal FLOPs for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflops\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/calflops/flops_counter.py:165\u001b[39m, in \u001b[36mcalculate_flops\u001b[39m\u001b[34m(model, input_shape, transformer_tokenizer, args, kwargs, forward_mode, include_backPropagation, compute_bp_factor, print_results, print_detailed, output_as_string, output_precision, output_unit, ignore_modules)\u001b[39m\n\u001b[32m    162\u001b[39m     args[index] = args[index].to(device)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m forward_mode == \u001b[33m'\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m forward_mode == \u001b[33m'\u001b[39m\u001b[33mgenerate\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    167\u001b[39m     _ = model.generate(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1558\u001b[39m     bw_hook = hooks.BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1559\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1561\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1563\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1564\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1565\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1566\u001b[39m     ):\n\u001b[32m   1567\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/SEMamba/models/generator.py:50\u001b[39m, in \u001b[36mSEMamba.forward\u001b[39m\u001b[34m(self, noisy_mag, noisy_pha)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03mForward pass for the SEMamba model.\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m \u001b[33;03m- denoised_com (torch.Tensor): Denoised complex tensor [B, F, T, 2].\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Reshape inputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m noisy_mag = \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_mag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mb f t -> b t f\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [B F T] -> [B, 1, T, F]\u001b[39;00m\n\u001b[32m     51\u001b[39m noisy_pha = rearrange(noisy_pha, \u001b[33m'\u001b[39m\u001b[33mb f t -> b t f\u001b[39m\u001b[33m'\u001b[39m).unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [B F T] -> [B, 1, T, F]\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Concatenate magnitude and phase inputs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/einops/einops.py:600\u001b[39m, in \u001b[36mrearrange\u001b[39m\u001b[34m(tensor, pattern, **axes_lengths)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, **axes_lengths: Size) -> Tensor:\n\u001b[32m    546\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[33;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m \n\u001b[32m    599\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrearrange\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/einops/einops.py:542\u001b[39m, in \u001b[36mreduce\u001b[39m\u001b[34m(tensor, pattern, reduction, **axes_lengths)\u001b[39m\n\u001b[32m    540\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Input is list. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    541\u001b[39m message += \u001b[33m\"\u001b[39m\u001b[33mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(axes_lengths)\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(e))\n",
      "\u001b[31mEinopsError\u001b[39m:  Error while processing rearrange-reduction pattern \"b f t -> b t f\".\n Input tensor shape: torch.Size([1, 1, 941, 257]). Additional info: {}.\n Wrong shape: expected 3 dims. Received 4-dim tensor."
     ]
    }
   ],
   "source": [
    "from calflops import calculate_flops\n",
    "with torch.no_grad():\n",
    "    # 使用 calflops 計算 FLOPs，將 args 改為列表\n",
    "        flops, macs, params = calculate_flops(\n",
    "            model=model,\n",
    "            args=[noisy_mag, noisy_pha],  # 使用列表而非元組\n",
    "            print_results=True  # 顯示逐層結果\n",
    "        )\n",
    "        print(f\"Total FLOPs for {fname}: {flops}\")\n",
    "        print(f\"Total Params: {params}\")\n",
    "        print(f\"Total MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops, macs, params = calculate_flops(\n",
    "    model=model,\n",
    "    args=[noisy_mag, noisy_pha],  # 使用列表而非元組\n",
    "    print_results=True  # 顯示逐層結果\n",
    ")\n",
    "print(f\"Total FLOPs for {fname}: {flops}\")\n",
    "print(f\"Total Params: {params}\")\n",
    "print(f\"Total MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk4/chocho/SEMamba/_test_feature_map_noisy p226_018.wav\n",
      "noisy_wav's shape: (94035,)\n",
      "norm_factor: 20.062986373901367\n",
      "noisy_wav's shape: torch.Size([1, 94035])\n",
      "x's shape: torch.Size([1, 2, 941, 257])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  51.78 K \n",
      "fwd MACs:                                                               3.02 GMACs\n",
      "fwd FLOPs:                                                              6.04 GFLOPS\n",
      "fwd+bwd MACs:                                                           9.07 GMACs\n",
      "fwd+bwd FLOPs:                                                          18.13 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "MambaBlock(\n",
      "  51.78 K = 100% Params, 3.02 GMACs = 100% MACs, 6.04 GFLOPS = 100% FLOPs\n",
      "  (forward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (backward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total FLOPs for p226_018.wav: 6.04 GFLOPS\n",
      "Total Params: 51.78 K\n",
      "Total MACs: 3.02 GMACs\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  51.78 K \n",
      "fwd MACs:                                                               3.02 GMACs\n",
      "fwd FLOPs:                                                              6.04 GFLOPS\n",
      "fwd+bwd MACs:                                                           9.07 GMACs\n",
      "fwd+bwd FLOPs:                                                          18.13 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "MambaBlock(\n",
      "  51.78 K = 100% Params, 3.02 GMACs = 100% MACs, 6.04 GFLOPS = 100% FLOPs\n",
      "  (forward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (backward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total FLOPs for p226_018.wav: 6.04 GFLOPS\n",
      "Total Params: 51.78 K\n",
      "Total MACs: 3.02 GMACs\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  51.78 K \n",
      "fwd MACs:                                                               3.02 GMACs\n",
      "fwd FLOPs:                                                              6.04 GFLOPS\n",
      "fwd+bwd MACs:                                                           9.07 GMACs\n",
      "fwd+bwd FLOPs:                                                          18.13 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "MambaBlock(\n",
      "  51.78 K = 100% Params, 3.02 GMACs = 100% MACs, 6.04 GFLOPS = 100% FLOPs\n",
      "  (forward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (backward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total FLOPs for p226_018.wav: 6.04 GFLOPS\n",
      "Total Params: 51.78 K\n",
      "Total MACs: 3.02 GMACs\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "---\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "x's shape: torch.Size([128, 941, 32])\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  51.78 K \n",
      "fwd MACs:                                                               3.02 GMACs\n",
      "fwd FLOPs:                                                              6.04 GFLOPS\n",
      "fwd+bwd MACs:                                                           9.07 GMACs\n",
      "fwd+bwd FLOPs:                                                          18.13 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "MambaBlock(\n",
      "  51.78 K = 100% Params, 3.02 GMACs = 100% MACs, 6.04 GFLOPS = 100% FLOPs\n",
      "  (forward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      "  (backward_blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      25.89 K = 50% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "      (mixer): Mamba(\n",
      "        25.86 K = 49.94% Params, 1.51 GMACs = 50% MACs, 3.02 GFLOPS = 50% FLOPs\n",
      "        (in_proj): Linear(8.19 K = 15.82% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "        (conv1d): Conv1d(512 = 0.99% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(3,), stride=(1,), padding=(2,), groups=128)\n",
      "        (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "        (x_proj): Linear(8.45 K = 16.32% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=66, bias=False)\n",
      "        (dt_proj): Linear(384 = 0.74% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "        (out_proj): Linear(4.1 K = 7.91% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm(32 = 0.06% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total FLOPs for p226_018.wav: 6.04 GFLOPS\n",
      "Total Params: 51.78 K\n",
      "Total MACs: 3.02 GMACs\n",
      "x's shape: torch.Size([941, 128, 32])\n",
      "x's shape: torch.Size([1, 32, 941, 128])\n",
      "---\n",
      "denoised_mag's shape: torch.Size([1, 257, 941])\n",
      "denoised_pha's shape: torch.Size([1, 257, 941])\n",
      "audio_g's shape: (94000,)\n"
     ]
    }
   ],
   "source": [
    "from c66 import pp, pps\n",
    "from einops import rearrange\n",
    "from calflops import calculate_flops\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for clean_or_noisy in  [\"clean\",\"noisy\"]:\n",
    "    for clean_or_noisy in  [\"noisy\"]:\n",
    "        input_folder = f'/disk4/chocho/SEMamba/_test_feature_map_{clean_or_noisy}'\n",
    "        for i, fname in enumerate(os.listdir( input_folder )):\n",
    "            print(input_folder, fname)\n",
    "            noisy_wav, _ = librosa.load(os.path.join( input_folder, fname ), sr=sampling_rate)\n",
    "            pps(noisy_wav)\n",
    "            noisy_wav = torch.FloatTensor(noisy_wav).to(device)\n",
    "            \n",
    "            norm_factor = torch.sqrt(len(noisy_wav) / torch.sum(noisy_wav ** 2.0)).to(device)\n",
    "            pp(norm_factor)\n",
    "            noisy_wav = (noisy_wav * norm_factor).unsqueeze(0)\n",
    "            pps(noisy_wav)\n",
    "            noisy_mag, noisy_pha, noisy_com = mag_phase_stft(noisy_wav, n_fft, hop_size, win_size, compress_factor)\n",
    "            \n",
    "            # feature_encoder\n",
    "            # Reshape inputs\n",
    "            noisy_mag = rearrange(noisy_mag, 'b f t -> b t f').unsqueeze(1)  # [B F T] -> [B, 1, T, F]\n",
    "            noisy_pha = rearrange(noisy_pha, 'b f t -> b t f').unsqueeze(1)  # [B F T] -> [B, 1, T, F]\n",
    "\n",
    "            # Concatenate magnitude and phase inputs\n",
    "            x = torch.cat((noisy_mag, noisy_pha), dim=1)  # [B, 2, T, F]\n",
    "            pps(x)\n",
    "\n",
    "            # Feature Encoder\n",
    "            x = feature_encoder(x)\n",
    "            pps(x)\n",
    "            \n",
    "            # TF-Mamba\n",
    "            # TSMamba is a instance of TFMambaBlock\n",
    "            for block in TSMamba:\n",
    "                b, c, t, f = x.size()\n",
    "                x = x.permute(0, 3, 2, 1).contiguous().view(b*f, t, c)\n",
    "                print(\"---\")\n",
    "                pps(x)\n",
    "                x = block.tlinear( block.time_mamba(x).permute(0,2,1) ).permute(0,2,1) + x\n",
    "                pps(x)\n",
    "                \n",
    "                flops, macs, params = calculate_flops(\n",
    "                    model=block.time_mamba,\n",
    "                    args=[x],  # 使用列表而非元組\n",
    "                    print_results=True  # 顯示逐層結果\n",
    "                )\n",
    "                print(f\"Total FLOPs for {fname}: {flops}\")\n",
    "                print(f\"Total Params: {params}\")\n",
    "                print(f\"Total MACs: {macs}\")\n",
    "                \n",
    "                x = x.view(b, f, t, c).permute(0, 2, 1, 3).contiguous().view(b*t, f, c)\n",
    "                x = block.flinear( block.freq_mamba(x).permute(0,2,1) ).permute(0,2,1) + x\n",
    "                pps(x)\n",
    "                x = x.view(b, t, f, c).permute(0, 3, 1, 2)\n",
    "                pps(x)\n",
    "                print(\"---\")\n",
    "\n",
    "            # Mag, Pha Decoder\n",
    "            denoised_mag = rearrange(mask_decoder(x) * noisy_mag, 'b c t f -> b f t c').squeeze(-1)\n",
    "            denoised_pha = rearrange(phase_decoder(x), 'b c t f -> b f t c').squeeze(-1)\n",
    "            pps(denoised_mag, denoised_pha)\n",
    "            \n",
    "            # \n",
    "            audio_g = mag_phase_istft(denoised_mag, denoised_pha, n_fft, hop_size, win_size, compress_factor)\n",
    "            audio_g = audio_g / norm_factor\n",
    "            audio_g = cal_pcs(audio_g.squeeze().cpu().numpy())\n",
    "            pps(audio_g)\n",
    "            \n",
    "            break\n",
    "            \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
