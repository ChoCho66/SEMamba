{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk4/chocho/SEMamba/_202505\n"
     ]
    }
   ],
   "source": [
    "cd _202505/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn  \n",
    "  \n",
    "class NeuralNetPyTorch1(nn.Module):  \n",
    "    def __init__(self,   \n",
    "                 input_neurons=72,  \n",
    "                 batchsize=1,   \n",
    "                 nDownSample=1,   \n",
    "                 kernel_size=6,   \n",
    "                 scalar_output=1.0):  \n",
    "        super(NeuralNetPyTorch1, self).__init__()  \n",
    "          \n",
    "        # 根據def_se_nn_arch72_mel.txt定義網絡結構  \n",
    "        self.neurons = [input_neurons, 72, 72, 72, 72, 257]  \n",
    "        self.layer_types = ['conv1d', 'lstm', 'fc', 'fc', 'fc']  \n",
    "        self.activations = ['tanh', 'tanh', 'tanh', 'tanh', 'sigmoid']  \n",
    "        self.dropprobs = [0.25, 0.25, 0.25, 0.25, 0.0]  # 最後一層沒有指定dropout，設為0  \n",
    "          \n",
    "        self.nDownSample = nDownSample  \n",
    "        self.kernel_size = kernel_size  \n",
    "        self.scalar_output = scalar_output  \n",
    "        self.num_layers = len(self.layer_types)  \n",
    "          \n",
    "        # 創建層列表  \n",
    "        self.layers = nn.ModuleList()  \n",
    "        self.dropout_layers = nn.ModuleList()  \n",
    "          \n",
    "        # LSTM 狀態變量  \n",
    "        self.h_states = [None] * self.num_layers  \n",
    "        self.c_states = [None] * self.num_layers  \n",
    "          \n",
    "        # 激活函數映射  \n",
    "        self.act_funcs = {  \n",
    "            'tanh': nn.Tanh(),  \n",
    "            'sigmoid': nn.Sigmoid(),  \n",
    "            'relu6': lambda x: torch.clamp(torch.relu(x), max=6.0),  \n",
    "            'linear': lambda x: x  \n",
    "        }  \n",
    "          \n",
    "        # 構建網絡層  \n",
    "        for i in range(self.num_layers):  \n",
    "            layer_type = self.layer_types[i]  \n",
    "            in_neurons = self.neurons[i]  \n",
    "            out_neurons = self.neurons[i+1]  \n",
    "              \n",
    "            if layer_type == 'fc':  \n",
    "                self.layers.append(nn.Linear(in_neurons, out_neurons))  \n",
    "                  \n",
    "            elif layer_type == 'lstm':  \n",
    "                self.layers.append(nn.LSTM(  \n",
    "                    input_size=in_neurons,  \n",
    "                    hidden_size=out_neurons,  \n",
    "                    batch_first=True,  \n",
    "                    num_layers=1  \n",
    "                ))  \n",
    "                  \n",
    "            elif layer_type == 'conv1d':  \n",
    "                # 注意：原始模型使用特殊配置的Conv2D作為1D卷積  \n",
    "                # 在PyTorch中，我們直接使用Conv1d  \n",
    "                self.layers.append(nn.Conv1d(  \n",
    "                    in_channels=in_neurons,  \n",
    "                    out_channels=out_neurons,  \n",
    "                    kernel_size=kernel_size,  \n",
    "                    stride=nDownSample,  \n",
    "                    padding='same'  # 保持輸出大小與輸入相同  \n",
    "                ))  \n",
    "              \n",
    "            # 添加dropout層  \n",
    "            self.dropout_layers.append(nn.Dropout(self.dropprobs[i]))  \n",
    "      \n",
    "    def forward(self, x, reset_states=False):  \n",
    "        batch_size = x.shape[0]  \n",
    "          \n",
    "        # 初始化或重置LSTM狀態  \n",
    "        if reset_states:  \n",
    "            for i in range(self.num_layers):  \n",
    "                if self.layer_types[i] == 'lstm':  \n",
    "                    self.h_states[i] = torch.zeros(1, batch_size, self.neurons[i+1], device=x.device)  \n",
    "                    self.c_states[i] = torch.zeros(1, batch_size, self.neurons[i+1], device=x.device)  \n",
    "          \n",
    "        # 前向傳播  \n",
    "        out = x  \n",
    "        for i in range(self.num_layers):  \n",
    "            layer = self.layers[i]  \n",
    "            layer_type = self.layer_types[i]  \n",
    "            activation = self.activations[i]  \n",
    "              \n",
    "            if layer_type == 'fc':  \n",
    "                out = layer(out)  \n",
    "                out = self.act_funcs[activation](out)  \n",
    "                out = self.dropout_layers[i](out)  \n",
    "                  \n",
    "            elif layer_type == 'lstm':  \n",
    "                if self.h_states[i] is not None and self.c_states[i] is not None:  \n",
    "                    # 使用先前的狀態  \n",
    "                    out, (self.h_states[i], self.c_states[i]) = layer(  \n",
    "                        out, (self.h_states[i], self.c_states[i])  \n",
    "                    )  \n",
    "                else:  \n",
    "                    # 沒有先前狀態  \n",
    "                    out, (self.h_states[i], self.c_states[i]) = layer(out)  \n",
    "                  \n",
    "                out = self.dropout_layers[i](out)  \n",
    "                  \n",
    "            elif layer_type == 'conv1d':  \n",
    "                # 調整形狀以適應Conv1d  \n",
    "                # Conv1d期望輸入形狀為 [batch_size, channels, length]  \n",
    "                # 而我們的輸入是 [batch_size, sequence_length, features]  \n",
    "                out = out.permute(0, 2, 1)  # 變為 [batch_size, features, sequence_length]  \n",
    "                out = layer(out)  \n",
    "                out = out.permute(0, 2, 1)  # 變回 [batch_size, sequence_length, features]  \n",
    "                out = self.act_funcs[activation](out)  \n",
    "                out = self.dropout_layers[i](out)  \n",
    "          \n",
    "        # 應用輸出縮放  \n",
    "        out = out * self.scalar_output  \n",
    "          \n",
    "        return out, (self.h_states, self.c_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReLU6(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(torch.relu(x), max=6.0)\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class NeuralNetPyTorch2(nn.Module):\n",
    "    def __init__(self, input_neurons=72, batchsize=1, nDownSample=1, kernel_size=6, scalar_output=1.0):\n",
    "        super(NeuralNetPyTorch2, self).__init__()\n",
    "        \n",
    "        self.neurons = [input_neurons, 72, 72, 72, 72, 257]\n",
    "        self.layer_types = ['conv1d', 'lstm', 'fc', 'fc', 'fc']\n",
    "        self.activations = ['tanh', 'tanh', 'tanh', 'tanh', 'sigmoid']\n",
    "        self.dropprobs = [0.25, 0.25, 0.25, 0.25, 0.0]\n",
    "        self.nDownSample = nDownSample\n",
    "        self.kernel_size = kernel_size\n",
    "        self.scalar_output = scalar_output\n",
    "        self.num_layers = len(self.layer_types)\n",
    "        \n",
    "        self.h_states = [None] * self.num_layers\n",
    "        self.c_states = [None] * self.num_layers\n",
    "        \n",
    "        # self.act_funcs = {\n",
    "        #     'tanh': nn.Tanh(),\n",
    "        #     'sigmoid': nn.Sigmoid(),\n",
    "        #     'relu6': lambda x: torch.clamp(torch.relu(x), max=6.0),\n",
    "        #     'linear': lambda x: x\n",
    "        # }\n",
    "        \n",
    "        self.act_funcs = {\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'relu6': ReLU6(),\n",
    "            'linear': Identity()\n",
    "        }\n",
    "        \n",
    "        # 使用 nn.Sequential 組織層\n",
    "        layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            layer_type = self.layer_types[i]\n",
    "            in_neurons = self.neurons[i]\n",
    "            out_neurons = self.neurons[i+1]\n",
    "            activation = self.act_funcs[self.activations[i]]\n",
    "            \n",
    "            if layer_type == 'fc':\n",
    "                layers.append(nn.Linear(in_neurons, out_neurons))\n",
    "                layers.append(activation)\n",
    "                if self.dropprobs[i] > 0:\n",
    "                    layers.append(nn.Dropout(self.dropprobs[i]))\n",
    "            \n",
    "            elif layer_type == 'conv1d':\n",
    "                layers.append(nn.Conv1d(in_channels=in_neurons, out_channels=out_neurons, kernel_size=kernel_size,\n",
    "                                        stride=nDownSample, padding='same'))\n",
    "                layers.append(activation)\n",
    "                if self.dropprobs[i] > 0:\n",
    "                    layers.append(nn.Dropout(self.dropprobs[i]))\n",
    "            \n",
    "            # LSTM 無法直接放入 nn.Sequential，因為它需要狀態管理\n",
    "            # 這裡我們暫時保留為單獨層\n",
    "            elif layer_type == 'lstm':\n",
    "                self.lstm_layer = nn.LSTM(input_size=in_neurons, hidden_size=out_neurons, batch_first=True, num_layers=1)\n",
    "                self.lstm_idx = i\n",
    "                layers.append(activation)\n",
    "                if self.dropprobs[i] > 0:\n",
    "                    layers.append(nn.Dropout(self.dropprobs[i]))\n",
    "        \n",
    "        self.sequential_layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, reset_states=False):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        if reset_states:\n",
    "            for i in range(self.num_layers):\n",
    "                if self.layer_types[i] == 'lstm':\n",
    "                    self.h_states[i] = torch.zeros(1, batch_size, self.neurons[i+1], device=x.device)\n",
    "                    self.c_states[i] = torch.zeros(1, batch_size, self.neurons[i+1], device=x.device)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            if self.layer_types[i] == 'conv1d':\n",
    "                out = out.permute(0, 2, 1)\n",
    "                out = self.sequential_layers[i*3:(i*3)+3](out)\n",
    "                out = out.permute(0, 2, 1)\n",
    "            elif self.layer_types[i] == 'lstm':\n",
    "                if self.h_states[i] is not None and self.c_states[i] is not None:\n",
    "                    out, (self.h_states[i], self.c_states[i]) = self.lstm_layer(out, (self.h_states[i], self.c_states[i]))\n",
    "                else:\n",
    "                    out, (self.h_states[i], self.c_states[i]) = self.lstm_layer(out)\n",
    "                out = self.sequential_layers[i*3:(i*3)+2](out)  # 應用激活和 Dropout\n",
    "            else:\n",
    "                out = self.sequential_layers[i*3:(i*3)+3](out)\n",
    "        \n",
    "        out = out * self.scalar_output\n",
    "        return out, (self.h_states, self.c_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在語音增強的實際應用中，這個過程如下：\n",
    "\n",
    "1. 將輸入音頻轉換為STFT頻譜\n",
    "1. 從STFT頻譜計算梅爾頻譜特徵（72維）\n",
    "1. 將這些特徵送入神經網絡\n",
    "1. 神經網絡輸出257維的時頻掩碼\n",
    "1. 將掩碼應用於原始STFT頻譜\n",
    "1. 進行逆STFT轉換，得到增強後的音頻\n",
    "\n",
    "### Notes\n",
    "\n",
    "- 輸入的72維特徵是經過降維的梅爾頻譜特徵，這種降維可以減少計算量並保留語音的關鍵特徵\n",
    "- 輸出的257維對應於原始STFT頻譜的頻率點數量，這樣掩碼可以直接應用於頻譜\n",
    "- 批次大小和序列長度在輸入和輸出之間保持不變，因為模型處理的是相同數量的時間步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入形狀: torch.Size([1, 1000, 72])\n",
      "輸出形狀: torch.Size([1, 1000, 257])\n",
      "tensor([[[0.4590, 0.5345, 0.4707,  ..., 0.5526, 0.5182, 0.4753],\n",
      "         [0.4552, 0.5107, 0.5023,  ..., 0.5745, 0.4943, 0.4710],\n",
      "         [0.4562, 0.5131, 0.4808,  ..., 0.5358, 0.4796, 0.4892],\n",
      "         ...,\n",
      "         [0.4596, 0.5322, 0.4696,  ..., 0.5226, 0.4915, 0.4806],\n",
      "         [0.4434, 0.5271, 0.4993,  ..., 0.5379, 0.4864, 0.4891],\n",
      "         [0.4629, 0.5078, 0.4882,  ..., 0.5508, 0.4919, 0.4887]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "輸入形狀: torch.Size([1, 1000, 72])\n",
      "輸出形狀: torch.Size([1, 1000, 257])\n",
      "tensor([[[0.4544, 0.4822, 0.5041,  ..., 0.5049, 0.4731, 0.4673],\n",
      "         [0.4659, 0.4680, 0.5000,  ..., 0.5083, 0.4918, 0.4866],\n",
      "         [0.4721, 0.4668, 0.4896,  ..., 0.4907, 0.4772, 0.4778],\n",
      "         ...,\n",
      "         [0.4596, 0.4753, 0.4686,  ..., 0.5078, 0.4724, 0.4816],\n",
      "         [0.4676, 0.4698, 0.4776,  ..., 0.4769, 0.4835, 0.4790],\n",
      "         [0.4702, 0.4775, 0.4993,  ..., 0.4789, 0.4856, 0.4861]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk4/chocho/speechbrain/.speechbrain/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /opt/conda/conda-bld/pytorch_1711403408687/work/aten/src/ATen/native/Convolution.cpp:1040.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 創建模型  \n",
    "model1 = NeuralNetPyTorch1(  \n",
    "    input_neurons=72,  \n",
    "    # nDownSample=1,  # 根據def_se_nn_arch72_mel.txt中的strides=1  \n",
    "    kernel_size=6,  # 根據def_se_nn_arch72_mel.txt中的kernel_size=6  \n",
    "    # scalar_output=1.0  \n",
    ").to(device)\n",
    "\n",
    "model2 = NeuralNetPyTorch2(  \n",
    "    input_neurons=72,  \n",
    "    # nDownSample=1,  # 根據def_se_nn_arch72_mel.txt中的strides=1  \n",
    "    kernel_size=6,  # 根據def_se_nn_arch72_mel.txt中的kernel_size=6  \n",
    "    # scalar_output=1.0  \n",
    ").to(device)\n",
    "\n",
    "# 假設的輸入 (batch_size, sequence_length, features)  \n",
    "x = torch.randn(1, 1000, 72).to(device)  # 批次大小為1，序列長度為10，特徵數為72  \n",
    "  \n",
    "# 前向傳播  \n",
    "output1, states = model1(x, reset_states=True)  \n",
    "print(f\"輸入形狀: {x.shape}\")  \n",
    "print(f\"輸出形狀: {output1.shape}\")  # 應該是 [1, 10, 257]\n",
    "print(output1)\n",
    "\n",
    "# 前向傳播  \n",
    "output2, states = model2(x, reset_states=True)  \n",
    "print(f\"輸入形狀: {x.shape}\")  \n",
    "print(f\"輸出形狀: {output2.shape}\")  # 應該是 [1, 10, 257]\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B,L,D: 1 1000 72\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "NeuralNetPyTorch1                        [1, 1000, 72]             [1, 1000, 257]            --\n",
      "├─ModuleList: 1-9                        --                        --                        (recursive)\n",
      "│    └─Conv1d: 2-1                       [1, 72, 1000]             [1, 72, 1000]             31,176\n",
      "├─ModuleList: 1-10                       --                        --                        --\n",
      "│    └─Dropout: 2-2                      [1, 1000, 72]             [1, 1000, 72]             --\n",
      "├─ModuleList: 1-9                        --                        --                        (recursive)\n",
      "│    └─LSTM: 2-3                         [1, 1000, 72]             [1, 1000, 72]             42,048\n",
      "├─ModuleList: 1-10                       --                        --                        --\n",
      "│    └─Dropout: 2-4                      [1, 1000, 72]             [1, 1000, 72]             --\n",
      "├─ModuleList: 1-9                        --                        --                        (recursive)\n",
      "│    └─Linear: 2-5                       [1, 1000, 72]             [1, 1000, 72]             5,256\n",
      "├─ModuleList: 1-10                       --                        --                        --\n",
      "│    └─Dropout: 2-6                      [1, 1000, 72]             [1, 1000, 72]             --\n",
      "├─ModuleList: 1-9                        --                        --                        (recursive)\n",
      "│    └─Linear: 2-7                       [1, 1000, 72]             [1, 1000, 72]             5,256\n",
      "├─ModuleList: 1-10                       --                        --                        --\n",
      "│    └─Dropout: 2-8                      [1, 1000, 72]             [1, 1000, 72]             --\n",
      "├─ModuleList: 1-9                        --                        --                        (recursive)\n",
      "│    └─Linear: 2-9                       [1, 1000, 72]             [1, 1000, 257]            18,761\n",
      "├─ModuleList: 1-10                       --                        --                        --\n",
      "│    └─Dropout: 2-10                     [1, 1000, 257]            [1, 1000, 257]            --\n",
      "===================================================================================================================\n",
      "Total params: 102,497\n",
      "Trainable params: 102,497\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 73.25\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.29\n",
      "Forward/backward pass size (MB): 4.36\n",
      "Params size (MB): 0.41\n",
      "Estimated Total Size (MB): 5.06\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "batch, length, dim = 1, 1000, 72\n",
    "print(\"B,L,D:\",batch, length, dim)\n",
    "x = torch.randn(batch, length, dim).to(device)\n",
    "\n",
    "# summary_str = summary(model, input_size=[(7, 201, 286), (7, 201, 286)], depth=5, col_names=(\"input_size\", \"output_size\", \"num_params\"), verbose=0)\n",
    "summary_str = summary(model1, input_size=[x.shape], depth=15, col_names=(\"input_size\", \"output_size\", \"num_params\"), verbose=0)\n",
    "print(summary_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B,L,D: 1 1000 72\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
      "===================================================================================================================\n",
      "NeuralNetPyTorch2                        [1, 1000, 72]             [1, 1000, 257]            5,256\n",
      "├─Sequential: 1-3                        --                        --                        (recursive)\n",
      "│    └─Conv1d: 2-1                       [1, 72, 1000]             [1, 72, 1000]             31,176\n",
      "│    └─Tanh: 2-2                         [1, 72, 1000]             [1, 72, 1000]             --\n",
      "│    └─Dropout: 2-3                      [1, 72, 1000]             [1, 72, 1000]             --\n",
      "├─LSTM: 1-2                              [1, 1000, 72]             [1, 1000, 72]             42,048\n",
      "├─Sequential: 1-3                        --                        --                        (recursive)\n",
      "│    └─Tanh: 2-4                         [1, 1000, 72]             [1, 1000, 72]             --\n",
      "│    └─Dropout: 2-5                      [1, 1000, 72]             [1, 1000, 72]             --\n",
      "│    └─Tanh: 2-6                         [1, 1000, 72]             [1, 1000, 72]             --\n",
      "│    └─Dropout: 2-7                      [1, 1000, 72]             [1, 1000, 72]             --\n",
      "│    └─Linear: 2-8                       [1, 1000, 72]             [1, 1000, 72]             5,256\n",
      "│    └─Tanh: 2-9                         [1, 1000, 72]             [1, 1000, 72]             --\n",
      "│    └─Dropout: 2-10                     [1, 1000, 72]             [1, 1000, 72]             --\n",
      "│    └─Linear: 2-11                      [1, 1000, 72]             [1, 1000, 257]            18,761\n",
      "│    └─Sigmoid: 2-12                     [1, 1000, 257]            [1, 1000, 257]            --\n",
      "===================================================================================================================\n",
      "Total params: 102,497\n",
      "Trainable params: 102,497\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 73.25\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.29\n",
      "Forward/backward pass size (MB): 3.78\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 4.46\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "batch, length, dim = 1, 1000, 72\n",
    "print(\"B,L,D:\",batch, length, dim)\n",
    "x = torch.randn(batch, length, dim).to(device)\n",
    "\n",
    "# summary_str = summary(model, input_size=[(7, 201, 286), (7, 201, 286)], depth=5, col_names=(\"input_size\", \"output_size\", \"num_params\"), verbose=0)\n",
    "summary_str = summary(model2, input_size=[x.shape], depth=15, col_names=(\"input_size\", \"output_size\", \"num_params\"), verbose=0)\n",
    "print(summary_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), 'model2_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408K\tmodel2_weights.pth\n"
     ]
    }
   ],
   "source": [
    "!du -sh model2_weights.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2, 'model2_full.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412K\tmodel2_full.pth\n"
     ]
    }
   ],
   "source": [
    "!du -sh model2_full.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
