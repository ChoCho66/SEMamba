{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# import argparse\n",
    "# import json\n",
    "# import torch\n",
    "# import librosa\n",
    "# from models.stfts import mag_phase_stft, mag_phase_istft\n",
    "# from models.generator import SEMamba\n",
    "# from models.pcs400 import cal_pcs\n",
    "# import soundfile as sf\n",
    "\n",
    "# from utils.util import (\n",
    "#     load_ckpts, load_optimizer_states, save_checkpoint,\n",
    "#     build_env, load_config, initialize_seed, \n",
    "#     print_gpu_info, log_model_info, initialize_process_group,\n",
    "# )\n",
    "\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generator import SEMamba\n",
    "import torch\n",
    "from utils.util import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Inference Process..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Initializing Inference Process..')\n",
    "# 修改 config 為有效的 YAML 配置文件路徑\n",
    "config = '/disk4/chocho/SEMamba/exp/20250403-SEMamba_v1_PCS-h32-tf2/config.yaml'  # 替換為實際的 YAML 檔案路徑\n",
    "checkpoint_file = '/disk4/chocho/SEMamba/exp/20250403-SEMamba_v1_PCS-h32-tf2/g_00078000.pth'\n",
    "\n",
    "global device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    #device = torch.device('cpu')\n",
    "    raise RuntimeError(\"Currently, CPU mode is not supported.\")\n",
    "\n",
    "cfg = load_config(config)\n",
    "model = SEMamba(cfg).to(device)\n",
    "state_dict = torch.load(checkpoint_file, map_location=device)\n",
    "model.load_state_dict(state_dict['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B,L,D: 5 201 286\n",
      "=============================================================================================================================\n",
      "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #\n",
      "=============================================================================================================================\n",
      "SEMamba                                            [5, 201, 286]             [5, 201, 286]             --\n",
      "├─DenseEncoder: 1-1                                [5, 2, 286, 201]          [5, 32, 286, 100]         --\n",
      "│    └─Sequential: 2-1                             [5, 2, 286, 201]          [5, 32, 286, 201]         --\n",
      "│    │    └─Conv2d: 3-1                            [5, 2, 286, 201]          [5, 32, 286, 201]         96\n",
      "│    │    └─InstanceNorm2d: 3-2                    [5, 32, 286, 201]         [5, 32, 286, 201]         64\n",
      "│    │    └─PReLU: 3-3                             [5, 32, 286, 201]         [5, 32, 286, 201]         32\n",
      "│    └─DenseBlock: 2-2                             [5, 32, 286, 201]         [5, 32, 286, 201]         --\n",
      "│    │    └─ModuleList: 3-4                        --                        --                        --\n",
      "│    │    │    └─Sequential: 4-1                   [5, 32, 286, 201]         [5, 32, 286, 201]         --\n",
      "│    │    │    │    └─Conv2d: 5-1                  [5, 32, 286, 201]         [5, 32, 286, 201]         9,248\n",
      "│    │    │    │    └─InstanceNorm2d: 5-2          [5, 32, 286, 201]         [5, 32, 286, 201]         64\n",
      "│    │    │    │    └─PReLU: 5-3                   [5, 32, 286, 201]         [5, 32, 286, 201]         32\n",
      "│    │    │    └─Sequential: 4-2                   [5, 64, 286, 201]         [5, 32, 286, 201]         --\n",
      "│    │    │    │    └─Conv2d: 5-4                  [5, 64, 286, 201]         [5, 32, 286, 201]         18,464\n",
      "│    │    │    │    └─InstanceNorm2d: 5-5          [5, 32, 286, 201]         [5, 32, 286, 201]         64\n",
      "│    │    │    │    └─PReLU: 5-6                   [5, 32, 286, 201]         [5, 32, 286, 201]         32\n",
      "│    │    │    └─Sequential: 4-3                   [5, 96, 286, 201]         [5, 32, 286, 201]         --\n",
      "│    │    │    │    └─Conv2d: 5-7                  [5, 96, 286, 201]         [5, 32, 286, 201]         27,680\n",
      "│    │    │    │    └─InstanceNorm2d: 5-8          [5, 32, 286, 201]         [5, 32, 286, 201]         64\n",
      "│    │    │    │    └─PReLU: 5-9                   [5, 32, 286, 201]         [5, 32, 286, 201]         32\n",
      "│    │    │    └─Sequential: 4-4                   [5, 128, 286, 201]        [5, 32, 286, 201]         --\n",
      "│    │    │    │    └─Conv2d: 5-10                 [5, 128, 286, 201]        [5, 32, 286, 201]         36,896\n",
      "│    │    │    │    └─InstanceNorm2d: 5-11         [5, 32, 286, 201]         [5, 32, 286, 201]         64\n",
      "│    │    │    │    └─PReLU: 5-12                  [5, 32, 286, 201]         [5, 32, 286, 201]         32\n",
      "│    └─Sequential: 2-3                             [5, 32, 286, 201]         [5, 32, 286, 100]         --\n",
      "│    │    └─Conv2d: 3-5                            [5, 32, 286, 201]         [5, 32, 286, 100]         3,104\n",
      "│    │    └─InstanceNorm2d: 3-6                    [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    └─PReLU: 3-7                             [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "├─ModuleList: 1-2                                  --                        --                        --\n",
      "│    └─TFMambaBlock: 2-4                           [5, 32, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    └─MambaBlock: 3-8                        [500, 286, 32]            [500, 286, 64]            --\n",
      "│    │    │    └─ModuleList: 4-5                   --                        --                        --\n",
      "│    │    │    │    └─Block: 5-13                  [500, 286, 32]            [500, 286, 32]            --\n",
      "│    │    │    │    │    └─RMSNorm: 6-1            [500, 286, 32]            [500, 286, 32]            32\n",
      "│    │    │    │    │    └─Mamba: 6-2              [500, 286, 32]            [500, 286, 32]            19,840\n",
      "│    │    │    └─ModuleList: 4-6                   --                        --                        --\n",
      "│    │    │    │    └─Block: 5-14                  [500, 286, 32]            [500, 286, 32]            --\n",
      "│    │    │    │    │    └─RMSNorm: 6-3            [500, 286, 32]            [500, 286, 32]            32\n",
      "│    │    │    │    │    └─Mamba: 6-4              [500, 286, 32]            [500, 286, 32]            19,840\n",
      "│    │    └─ConvTranspose1d: 3-9                   [500, 64, 286]            [500, 32, 286]            2,080\n",
      "│    │    └─MambaBlock: 3-10                       [1430, 100, 32]           [1430, 100, 64]           --\n",
      "│    │    │    └─ModuleList: 4-7                   --                        --                        --\n",
      "│    │    │    │    └─Block: 5-15                  [1430, 100, 32]           [1430, 100, 32]           --\n",
      "│    │    │    │    │    └─RMSNorm: 6-5            [1430, 100, 32]           [1430, 100, 32]           32\n",
      "│    │    │    │    │    └─Mamba: 6-6              [1430, 100, 32]           [1430, 100, 32]           19,840\n",
      "│    │    │    └─ModuleList: 4-8                   --                        --                        --\n",
      "│    │    │    │    └─Block: 5-16                  [1430, 100, 32]           [1430, 100, 32]           --\n",
      "│    │    │    │    │    └─RMSNorm: 6-7            [1430, 100, 32]           [1430, 100, 32]           32\n",
      "│    │    │    │    │    └─Mamba: 6-8              [1430, 100, 32]           [1430, 100, 32]           19,840\n",
      "│    │    └─ConvTranspose1d: 3-11                  [1430, 64, 100]           [1430, 32, 100]           2,080\n",
      "│    └─TFMambaBlock: 2-5                           [5, 32, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    └─MambaBlock: 3-12                       [500, 286, 32]            [500, 286, 64]            --\n",
      "│    │    │    └─ModuleList: 4-9                   --                        --                        --\n",
      "│    │    │    │    └─Block: 5-17                  [500, 286, 32]            [500, 286, 32]            --\n",
      "│    │    │    │    │    └─RMSNorm: 6-9            [500, 286, 32]            [500, 286, 32]            32\n",
      "│    │    │    │    │    └─Mamba: 6-10             [500, 286, 32]            [500, 286, 32]            19,840\n",
      "│    │    │    └─ModuleList: 4-10                  --                        --                        --\n",
      "│    │    │    │    └─Block: 5-18                  [500, 286, 32]            [500, 286, 32]            --\n",
      "│    │    │    │    │    └─RMSNorm: 6-11           [500, 286, 32]            [500, 286, 32]            32\n",
      "│    │    │    │    │    └─Mamba: 6-12             [500, 286, 32]            [500, 286, 32]            19,840\n",
      "│    │    └─ConvTranspose1d: 3-13                  [500, 64, 286]            [500, 32, 286]            2,080\n",
      "│    │    └─MambaBlock: 3-14                       [1430, 100, 32]           [1430, 100, 64]           --\n",
      "│    │    │    └─ModuleList: 4-11                  --                        --                        --\n",
      "│    │    │    │    └─Block: 5-19                  [1430, 100, 32]           [1430, 100, 32]           --\n",
      "│    │    │    │    │    └─RMSNorm: 6-13           [1430, 100, 32]           [1430, 100, 32]           32\n",
      "│    │    │    │    │    └─Mamba: 6-14             [1430, 100, 32]           [1430, 100, 32]           19,840\n",
      "│    │    │    └─ModuleList: 4-12                  --                        --                        --\n",
      "│    │    │    │    └─Block: 5-20                  [1430, 100, 32]           [1430, 100, 32]           --\n",
      "│    │    │    │    │    └─RMSNorm: 6-15           [1430, 100, 32]           [1430, 100, 32]           32\n",
      "│    │    │    │    │    └─Mamba: 6-16             [1430, 100, 32]           [1430, 100, 32]           19,840\n",
      "│    │    └─ConvTranspose1d: 3-15                  [1430, 64, 100]           [1430, 32, 100]           2,080\n",
      "├─MagDecoder: 1-3                                  [5, 32, 286, 100]         [5, 1, 286, 201]          --\n",
      "│    └─DenseBlock: 2-6                             [5, 32, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    └─ModuleList: 3-16                       --                        --                        --\n",
      "│    │    │    └─Sequential: 4-13                  [5, 32, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-21                 [5, 32, 286, 100]         [5, 32, 286, 100]         9,248\n",
      "│    │    │    │    └─InstanceNorm2d: 5-22         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-23                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    │    │    └─Sequential: 4-14                  [5, 64, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-24                 [5, 64, 286, 100]         [5, 32, 286, 100]         18,464\n",
      "│    │    │    │    └─InstanceNorm2d: 5-25         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-26                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    │    │    └─Sequential: 4-15                  [5, 96, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-27                 [5, 96, 286, 100]         [5, 32, 286, 100]         27,680\n",
      "│    │    │    │    └─InstanceNorm2d: 5-28         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-29                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    │    │    └─Sequential: 4-16                  [5, 128, 286, 100]        [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-30                 [5, 128, 286, 100]        [5, 32, 286, 100]         36,896\n",
      "│    │    │    │    └─InstanceNorm2d: 5-31         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-32                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    └─Sequential: 2-7                             [5, 32, 286, 100]         [5, 1, 286, 201]          --\n",
      "│    │    └─ConvTranspose2d: 3-17                  [5, 32, 286, 100]         [5, 32, 286, 201]         3,104\n",
      "│    │    └─Conv2d: 3-18                           [5, 32, 286, 201]         [5, 1, 286, 201]          33\n",
      "│    │    └─InstanceNorm2d: 3-19                   [5, 1, 286, 201]          [5, 1, 286, 201]          2\n",
      "│    │    └─PReLU: 3-20                            [5, 1, 286, 201]          [5, 1, 286, 201]          1\n",
      "│    │    └─Conv2d: 3-21                           [5, 1, 286, 201]          [5, 1, 286, 201]          2\n",
      "│    └─LearnableSigmoid2D: 2-8                     [5, 201, 286]             [5, 201, 286]             201\n",
      "├─PhaseDecoder: 1-4                                [5, 32, 286, 100]         [5, 1, 286, 201]          --\n",
      "│    └─DenseBlock: 2-9                             [5, 32, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    └─ModuleList: 3-22                       --                        --                        --\n",
      "│    │    │    └─Sequential: 4-17                  [5, 32, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-33                 [5, 32, 286, 100]         [5, 32, 286, 100]         9,248\n",
      "│    │    │    │    └─InstanceNorm2d: 5-34         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-35                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    │    │    └─Sequential: 4-18                  [5, 64, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-36                 [5, 64, 286, 100]         [5, 32, 286, 100]         18,464\n",
      "│    │    │    │    └─InstanceNorm2d: 5-37         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-38                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    │    │    └─Sequential: 4-19                  [5, 96, 286, 100]         [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-39                 [5, 96, 286, 100]         [5, 32, 286, 100]         27,680\n",
      "│    │    │    │    └─InstanceNorm2d: 5-40         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-41                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    │    │    └─Sequential: 4-20                  [5, 128, 286, 100]        [5, 32, 286, 100]         --\n",
      "│    │    │    │    └─Conv2d: 5-42                 [5, 128, 286, 100]        [5, 32, 286, 100]         36,896\n",
      "│    │    │    │    └─InstanceNorm2d: 5-43         [5, 32, 286, 100]         [5, 32, 286, 100]         64\n",
      "│    │    │    │    └─PReLU: 5-44                  [5, 32, 286, 100]         [5, 32, 286, 100]         32\n",
      "│    └─Sequential: 2-10                            [5, 32, 286, 100]         [5, 32, 286, 201]         --\n",
      "│    │    └─ConvTranspose2d: 3-23                  [5, 32, 286, 100]         [5, 32, 286, 201]         3,104\n",
      "│    │    └─InstanceNorm2d: 3-24                   [5, 32, 286, 201]         [5, 32, 286, 201]         64\n",
      "│    │    └─PReLU: 3-25                            [5, 32, 286, 201]         [5, 32, 286, 201]         32\n",
      "│    └─Conv2d: 2-11                                [5, 32, 286, 201]         [5, 1, 286, 201]          33\n",
      "│    └─Conv2d: 2-12                                [5, 32, 286, 201]         [5, 1, 286, 201]          33\n",
      "=============================================================================================================================\n",
      "Total params: 455,313\n",
      "Trainable params: 455,313\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 56.40\n",
      "=============================================================================================================================\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 2841.87\n",
      "Params size (MB): 1.19\n",
      "Estimated Total Size (MB): 2845.35\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# from mamba_ssm.modules.mamba_simple import Mamba\n",
    "import torch\n",
    "# from mamba_ssm import Mamba\n",
    "from torchinfo import summary\n",
    "# from torchprofile import profile_macs\n",
    "\n",
    "batch, length, dim = 5, 201, 286\n",
    "print(\"B,L,D:\",batch, length, dim)\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "\n",
    "# summary_str = summary(model, input_size=[(7, 201, 286), (7, 201, 286)], depth=5, col_names=(\"input_size\", \"output_size\", \"num_params\"), verbose=0)\n",
    "summary_str = summary(model, input_size=[x.shape, x.shape], depth=15, col_names=(\"input_size\", \"output_size\", \"num_params\"), verbose=0)\n",
    "print(summary_str)\n",
    "\n",
    "# y = model(x)\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B,L,D: 5 201 286\n",
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  455.31 K\n",
      "fwd MACs:                                                               65.05 GMACs\n",
      "fwd FLOPs:                                                              130.79 GFLOPS\n",
      "fwd+bwd MACs:                                                           195.14 GMACs\n",
      "fwd+bwd FLOPs:                                                          392.36 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "SEMamba(\n",
      "  455.31 K = 100% Params, 65.05 GMACs = 100% MACs, 130.79 GFLOPS = 100% FLOPs\n",
      "  (dense_encoder): DenseEncoder(\n",
      "    96.06 K = 21.1% Params, 26.95 GMACs = 41.43% MACs, 54.25 GFLOPS = 41.48% FLOPs\n",
      "    (dense_conv_1): Sequential(\n",
      "      192 = 0.04% Params, 18.4 MMACs = 0.03% MACs, 101.18 MFLOPS = 0.08% FLOPs\n",
      "      (0): Conv2d(96 = 0.02% Params, 18.4 MMACs = 0.03% MACs, 45.99 MFLOPS = 0.04% FLOPs, 2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.04% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.01% FLOPs, num_parameters=32)\n",
      "    )\n",
      "    (dense_block): DenseBlock(\n",
      "      92.67 K = 20.35% Params, 26.49 GMACs = 40.72% MACs, 53.24 GFLOPS = 40.71% FLOPs\n",
      "      (dense_block): ModuleList(\n",
      "        (0): Sequential(\n",
      "          9.34 K = 2.05% Params, 2.65 GMACs = 4.07% MACs, 5.36 GFLOPS = 4.1% FLOPs\n",
      "          (0): Conv2d(9.25 K = 2.03% Params, 2.65 GMACs = 4.07% MACs, 5.31 GFLOPS = 4.06% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.04% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.01% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          18.56 K = 4.08% Params, 5.3 GMACs = 8.14% MACs, 10.66 GFLOPS = 8.15% FLOPs\n",
      "          (0): Conv2d(18.46 K = 4.06% Params, 5.3 GMACs = 8.14% MACs, 10.61 GFLOPS = 8.11% FLOPs, 64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.04% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.01% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          27.78 K = 6.1% Params, 7.95 GMACs = 12.22% MACs, 15.96 GFLOPS = 12.2% FLOPs\n",
      "          (0): Conv2d(27.68 K = 6.08% Params, 7.95 GMACs = 12.22% MACs, 15.9 GFLOPS = 12.16% FLOPs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.04% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.01% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          36.99 K = 8.12% Params, 10.6 GMACs = 16.29% MACs, 21.26 GFLOPS = 16.25% FLOPs\n",
      "          (0): Conv2d(36.9 K = 8.1% Params, 10.6 GMACs = 16.29% MACs, 21.2 GFLOPS = 16.21% FLOPs, 128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.04% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.01% FLOPs, num_parameters=32)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dense_conv_2): Sequential(\n",
      "      3.2 K = 0.7% Params, 439.3 MMACs = 0.68% MACs, 910.62 MFLOPS = 0.7% FLOPs\n",
      "      (0): Conv2d(3.1 K = 0.68% Params, 439.3 MMACs = 0.68% MACs, 883.17 MFLOPS = 0.68% FLOPs, 32, 32, kernel_size=(1, 3), stride=(1, 2))\n",
      "      (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "    )\n",
      "  )\n",
      "  (TSMamba): ModuleList(\n",
      "    (0-1): 2 x TFMambaBlock(\n",
      "      83.65 K = 18.37% Params, 5.42 GMACs = 8.33% MACs, 10.85 GFLOPS = 8.29% FLOPs\n",
      "      (time_mamba): MambaBlock(\n",
      "        39.74 K = 8.73% Params, 2.42 GMACs = 3.71% MACs, 4.83 GFLOPS = 3.69% FLOPs\n",
      "        (forward_blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            19.87 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "            (mixer): Mamba(\n",
      "              19.84 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "              (in_proj): Linear(8.19 K = 1.8% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "              (conv1d): Conv1d(640 = 0.14% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "              (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "              (x_proj): Linear(4.35 K = 0.96% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=34, bias=False)\n",
      "              (dt_proj): Linear(384 = 0.08% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "              (out_proj): Linear(4.1 K = 0.9% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "            )\n",
      "            (norm): RMSNorm(32 = 0.01% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "        )\n",
      "        (backward_blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            19.87 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "            (mixer): Mamba(\n",
      "              19.84 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "              (in_proj): Linear(8.19 K = 1.8% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "              (conv1d): Conv1d(640 = 0.14% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "              (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "              (x_proj): Linear(4.35 K = 0.96% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=34, bias=False)\n",
      "              (dt_proj): Linear(384 = 0.08% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "              (out_proj): Linear(4.1 K = 0.9% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "            )\n",
      "            (norm): RMSNorm(32 = 0.01% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (freq_mamba): MambaBlock(\n",
      "        39.74 K = 8.73% Params, 2.42 GMACs = 3.71% MACs, 4.83 GFLOPS = 3.69% FLOPs\n",
      "        (forward_blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            19.87 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "            (mixer): Mamba(\n",
      "              19.84 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "              (in_proj): Linear(8.19 K = 1.8% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "              (conv1d): Conv1d(640 = 0.14% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "              (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "              (x_proj): Linear(4.35 K = 0.96% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=34, bias=False)\n",
      "              (dt_proj): Linear(384 = 0.08% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "              (out_proj): Linear(4.1 K = 0.9% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "            )\n",
      "            (norm): RMSNorm(32 = 0.01% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "        )\n",
      "        (backward_blocks): ModuleList(\n",
      "          (0): Block(\n",
      "            19.87 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "            (mixer): Mamba(\n",
      "              19.84 K = 4.36% Params, 1.21 GMACs = 1.86% MACs, 2.42 GFLOPS = 1.85% FLOPs\n",
      "              (in_proj): Linear(8.19 K = 1.8% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=32, out_features=256, bias=False)\n",
      "              (conv1d): Conv1d(640 = 0.14% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
      "              (act): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "              (x_proj): Linear(4.35 K = 0.96% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=34, bias=False)\n",
      "              (dt_proj): Linear(384 = 0.08% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=2, out_features=128, bias=True)\n",
      "              (out_proj): Linear(4.1 K = 0.9% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=128, out_features=32, bias=False)\n",
      "            )\n",
      "            (norm): RMSNorm(32 = 0.01% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (tlinear): ConvTranspose1d(2.08 K = 0.46% Params, 292.86 MMACs = 0.45% MACs, 590.3 MFLOPS = 0.45% FLOPs, 64, 32, kernel_size=(1,), stride=(1,))\n",
      "      (flinear): ConvTranspose1d(2.08 K = 0.46% Params, 292.86 MMACs = 0.45% MACs, 590.3 MFLOPS = 0.45% FLOPs, 64, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (mask_decoder): MagDecoder(\n",
      "    96.02 K = 21.09% Params, 13.63 GMACs = 20.95% MACs, 27.39 GFLOPS = 20.94% FLOPs\n",
      "    (dense_block): DenseBlock(\n",
      "      92.67 K = 20.35% Params, 13.18 GMACs = 20.26% MACs, 26.49 GFLOPS = 20.25% FLOPs\n",
      "      (dense_block): ModuleList(\n",
      "        (0): Sequential(\n",
      "          9.34 K = 2.05% Params, 1.32 GMACs = 2.03% MACs, 2.67 GFLOPS = 2.04% FLOPs\n",
      "          (0): Conv2d(9.25 K = 2.03% Params, 1.32 GMACs = 2.03% MACs, 2.64 GFLOPS = 2.02% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          18.56 K = 4.08% Params, 2.64 GMACs = 4.05% MACs, 5.3 GFLOPS = 4.06% FLOPs\n",
      "          (0): Conv2d(18.46 K = 4.06% Params, 2.64 GMACs = 4.05% MACs, 5.28 GFLOPS = 4.03% FLOPs, 64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          27.78 K = 6.1% Params, 3.95 GMACs = 6.08% MACs, 7.94 GFLOPS = 6.07% FLOPs\n",
      "          (0): Conv2d(27.68 K = 6.08% Params, 3.95 GMACs = 6.08% MACs, 7.91 GFLOPS = 6.05% FLOPs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          36.99 K = 8.12% Params, 5.27 GMACs = 8.1% MACs, 10.58 GFLOPS = 8.09% FLOPs\n",
      "          (0): Conv2d(36.9 K = 8.1% Params, 5.27 GMACs = 8.1% MACs, 10.55 GFLOPS = 8.06% FLOPs, 128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mask_conv): Sequential(\n",
      "      3.14 K = 0.69% Params, 448.78 MMACs = 0.69% MACs, 902.1 MFLOPS = 0.69% FLOPs\n",
      "      (0): ConvTranspose2d(3.1 K = 0.68% Params, 439.3 MMACs = 0.68% MACs, 880.83 MFLOPS = 0.67% FLOPs, 32, 32, kernel_size=(1, 3), stride=(1, 2))\n",
      "      (1): Conv2d(33 = 0.01% Params, 9.2 MMACs = 0.01% MACs, 18.68 MFLOPS = 0.01% FLOPs, 32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): InstanceNorm2d(2 = 0% Params, 0 MACs = 0% MACs, 1.44 MFLOPS = 0% FLOPs, 1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (3): PReLU(1 = 0% Params, 0 MACs = 0% MACs, 287.43 KFLOPS = 0% FLOPs, num_parameters=1)\n",
      "      (4): Conv2d(2 = 0% Params, 287.43 KMACs = 0% MACs, 862.29 KFLOPS = 0% FLOPs, 1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (lsigmoid): LearnableSigmoid2D(201 = 0.04% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "  )\n",
      "  (phase_decoder): PhaseDecoder(\n",
      "    95.94 K = 21.07% Params, 13.64 GMACs = 20.96% MACs, 27.46 GFLOPS = 21% FLOPs\n",
      "    (dense_block): DenseBlock(\n",
      "      92.67 K = 20.35% Params, 13.18 GMACs = 20.26% MACs, 26.49 GFLOPS = 20.25% FLOPs\n",
      "      (dense_block): ModuleList(\n",
      "        (0): Sequential(\n",
      "          9.34 K = 2.05% Params, 1.32 GMACs = 2.03% MACs, 2.67 GFLOPS = 2.04% FLOPs\n",
      "          (0): Conv2d(9.25 K = 2.03% Params, 1.32 GMACs = 2.03% MACs, 2.64 GFLOPS = 2.02% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          18.56 K = 4.08% Params, 2.64 GMACs = 4.05% MACs, 5.3 GFLOPS = 4.06% FLOPs\n",
      "          (0): Conv2d(18.46 K = 4.06% Params, 2.64 GMACs = 4.05% MACs, 5.28 GFLOPS = 4.03% FLOPs, 64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          27.78 K = 6.1% Params, 3.95 GMACs = 6.08% MACs, 7.94 GFLOPS = 6.07% FLOPs\n",
      "          (0): Conv2d(27.68 K = 6.08% Params, 3.95 GMACs = 6.08% MACs, 7.91 GFLOPS = 6.05% FLOPs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          36.99 K = 8.12% Params, 5.27 GMACs = 8.1% MACs, 10.58 GFLOPS = 8.09% FLOPs\n",
      "          (0): Conv2d(36.9 K = 8.1% Params, 5.27 GMACs = 8.1% MACs, 10.55 GFLOPS = 8.06% FLOPs, 128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
      "          (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.02% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0% FLOPs, num_parameters=32)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (phase_conv): Sequential(\n",
      "      3.2 K = 0.7% Params, 439.3 MMACs = 0.68% MACs, 936.02 MFLOPS = 0.72% FLOPs\n",
      "      (0): ConvTranspose2d(3.1 K = 0.68% Params, 439.3 MMACs = 0.68% MACs, 880.83 MFLOPS = 0.67% FLOPs, 32, 32, kernel_size=(1, 3), stride=(1, 2))\n",
      "      (1): InstanceNorm2d(64 = 0.01% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.04% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): PReLU(32 = 0.01% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.01% FLOPs, num_parameters=32)\n",
      "    )\n",
      "    (phase_conv_r): Conv2d(33 = 0.01% Params, 9.2 MMACs = 0.01% MACs, 18.68 MFLOPS = 0.01% FLOPs, 32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (phase_conv_i): Conv2d(33 = 0.01% Params, 9.2 MMACs = 0.01% MACs, 18.68 MFLOPS = 0.01% FLOPs, 32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total FLOPs: 130.79 GFLOPS\n",
      "Total Params: 455.31 K\n",
      "Total MACs: 65.05 GMACs\n"
     ]
    }
   ],
   "source": [
    "from calflops import calculate_flops\n",
    "\n",
    "batch, length, dim = 5, 201, 286\n",
    "print(\"B,L,D:\",batch, length, dim)\n",
    "noisy_amp = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "noisy_pha = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 使用 calflops 計算 FLOPs，將 args 改為列表\n",
    "    flops, macs, params = calculate_flops(\n",
    "        model=model,\n",
    "        args=[noisy_amp, noisy_pha],  # 使用列表而非元組\n",
    "        print_results=True  # 顯示逐層結果\n",
    "    )\n",
    "    # print(f\"Total FLOPs for {fname}: {flops / 1e9:.3f} GFLOPs\")\n",
    "    # print(f\"Total Params: {params / 1e6:.3f} M\")\n",
    "    # print(f\"Total MACs: {macs / 1e9:.3f} GMACs\")\n",
    "    print(f\"Total FLOPs: {flops}\")\n",
    "    print(f\"Total Params: {params}\")\n",
    "    print(f\"Total MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  96.06 K \n",
      "fwd MACs:                                                               26.95 GMACs\n",
      "fwd FLOPs:                                                              54.25 GFLOPS\n",
      "fwd+bwd MACs:                                                           80.84 GMACs\n",
      "fwd+bwd FLOPs:                                                          162.75 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "DenseEncoder(\n",
      "  96.06 K = 100% Params, 26.95 GMACs = 100% MACs, 54.25 GFLOPS = 100% FLOPs\n",
      "  (dense_conv_1): Sequential(\n",
      "    192 = 0.2% Params, 18.4 MMACs = 0.07% MACs, 101.18 MFLOPS = 0.19% FLOPs\n",
      "    (0): Conv2d(96 = 0.1% Params, 18.4 MMACs = 0.07% MACs, 45.99 MFLOPS = 0.08% FLOPs, 2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): InstanceNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.08% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (2): PReLU(32 = 0.03% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.02% FLOPs, num_parameters=32)\n",
      "  )\n",
      "  (dense_block): DenseBlock(\n",
      "    92.67 K = 96.47% Params, 26.49 GMACs = 98.3% MACs, 53.24 GFLOPS = 98.13% FLOPs\n",
      "    (dense_block): ModuleList(\n",
      "      (0): Sequential(\n",
      "        9.34 K = 9.73% Params, 2.65 GMACs = 9.83% MACs, 5.36 GFLOPS = 9.88% FLOPs\n",
      "        (0): Conv2d(9.25 K = 9.63% Params, 2.65 GMACs = 9.83% MACs, 5.31 GFLOPS = 9.78% FLOPs, 32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): InstanceNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.08% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (2): PReLU(32 = 0.03% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.02% FLOPs, num_parameters=32)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        18.56 K = 19.32% Params, 5.3 GMACs = 19.66% MACs, 10.66 GFLOPS = 19.65% FLOPs\n",
      "        (0): Conv2d(18.46 K = 19.22% Params, 5.3 GMACs = 19.66% MACs, 10.61 GFLOPS = 19.55% FLOPs, 64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 1), dilation=(2, 1))\n",
      "        (1): InstanceNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.08% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (2): PReLU(32 = 0.03% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.02% FLOPs, num_parameters=32)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        27.78 K = 28.91% Params, 7.95 GMACs = 29.49% MACs, 15.96 GFLOPS = 29.42% FLOPs\n",
      "        (0): Conv2d(27.68 K = 28.81% Params, 7.95 GMACs = 29.49% MACs, 15.9 GFLOPS = 29.31% FLOPs, 96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 1), dilation=(4, 1))\n",
      "        (1): InstanceNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.08% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (2): PReLU(32 = 0.03% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.02% FLOPs, num_parameters=32)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        36.99 K = 38.51% Params, 10.6 GMACs = 39.32% MACs, 21.26 GFLOPS = 39.18% FLOPs\n",
      "        (0): Conv2d(36.9 K = 38.41% Params, 10.6 GMACs = 39.32% MACs, 21.2 GFLOPS = 39.08% FLOPs, 128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 1), dilation=(8, 1))\n",
      "        (1): InstanceNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 45.99 MFLOPS = 0.08% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        (2): PReLU(32 = 0.03% Params, 0 MACs = 0% MACs, 9.2 MFLOPS = 0.02% FLOPs, num_parameters=32)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dense_conv_2): Sequential(\n",
      "    3.2 K = 3.33% Params, 439.3 MMACs = 1.63% MACs, 910.62 MFLOPS = 1.68% FLOPs\n",
      "    (0): Conv2d(3.1 K = 3.23% Params, 439.3 MMACs = 1.63% MACs, 883.17 MFLOPS = 1.63% FLOPs, 32, 32, kernel_size=(1, 3), stride=(1, 2))\n",
      "    (1): InstanceNorm2d(64 = 0.07% Params, 0 MACs = 0% MACs, 22.88 MFLOPS = 0.04% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    (2): PReLU(32 = 0.03% Params, 0 MACs = 0% MACs, 4.58 MFLOPS = 0.01% FLOPs, num_parameters=32)\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Total FLOPs: 54.25 GFLOPS\n",
      "Total Params: 96.06 K\n",
      "Total MACs: 26.95 GMACs\n"
     ]
    }
   ],
   "source": [
    "from calflops import calculate_flops\n",
    "\n",
    "shape = [5,2,286,201]\n",
    "with torch.no_grad():\n",
    "    # 使用 calflops 計算 FLOPs，將 args 改為列表\n",
    "    flops, macs, params = calculate_flops(\n",
    "        model=model.dense_encoder,\n",
    "        args=[torch.randn(shape).to(\"cuda\")],  # 使用列表而非元組\n",
    "        print_results=True  # 顯示逐層結果\n",
    "    )\n",
    "    # print(f\"Total FLOPs for {fname}: {flops / 1e9:.3f} GFLOPs\")\n",
    "    # print(f\"Total Params: {params / 1e6:.3f} M\")\n",
    "    # print(f\"Total MACs: {macs / 1e9:.3f} GMACs\")\n",
    "    print(f\"Total FLOPs: {flops}\")\n",
    "    print(f\"Total Params: {params}\")\n",
    "    print(f\"Total MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (2): PReLU(num_parameters=32)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_encoder.dense_conv_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 2, 1, 1]), torch.Size([32]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_encoder.dense_conv_1[0].weight.shape, \\\n",
    "    model.dense_encoder.dense_conv_1[0].bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]), torch.Size([32]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_encoder.dense_conv_1[1].weight.shape, \\\n",
    "    model.dense_encoder.dense_conv_1[1].bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_encoder.dense_conv_1[2].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.dense_encoder.dense_conv_1[1].weight.data = torch.ones_like(model.dense_encoder.dense_conv_1[1].weight)\n",
    "model.dense_encoder.dense_conv_1[1].weight    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        device='cuda:0', requires_grad=True),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_encoder.dense_conv_1[1].weight,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([-0.0352, -0.0052, -0.0340, -0.0106,  0.1124,  0.2487,  0.1829,  0.0049,\n",
       "         -0.0568,  0.2071, -0.0741,  0.1177,  0.1433, -0.0592,  0.0790, -0.0420,\n",
       "          0.0500,  0.3584,  0.0108,  0.1113, -0.0980,  0.0066,  0.1041, -0.0105,\n",
       "          0.1420,  0.3210,  0.0218, -0.0035, -0.0601, -0.2030,  0.1472, -0.0635],\n",
       "        device='cuda:0', requires_grad=True),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_encoder.dense_conv_1[2].weight,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(\n",
       "  (mixer): Mamba(\n",
       "    (in_proj): Linear(in_features=32, out_features=256, bias=False)\n",
       "    (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
       "    (act): SiLU()\n",
       "    (x_proj): Linear(in_features=128, out_features=34, bias=False)\n",
       "    (dt_proj): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (out_proj): Linear(in_features=128, out_features=32, bias=False)\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.TSMamba[0].time_mamba.forward_blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSNorm()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.TSMamba[0].time_mamba.forward_blocks[0].norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mamba(\n",
       "  (in_proj): Linear(in_features=32, out_features=256, bias=False)\n",
       "  (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
       "  (act): SiLU()\n",
       "  (x_proj): Linear(in_features=128, out_features=34, bias=False)\n",
       "  (dt_proj): Linear(in_features=2, out_features=128, bias=True)\n",
       "  (out_proj): Linear(in_features=128, out_features=32, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.TSMamba[0].time_mamba.forward_blocks[0].mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 7.1235e-03, -1.0601e-02,  2.9499e-01,  ...,  1.1048e-01,\n",
       "             1.1252e-01, -2.2514e-01],\n",
       "           [ 2.2336e-01,  2.5662e-01,  1.0795e-01,  ...,  1.7424e-01,\n",
       "             2.3170e-01, -1.1843e-01],\n",
       "           [ 1.0530e-01,  4.6253e-02,  2.5955e-01,  ..., -6.2019e-02,\n",
       "             1.2044e-01,  1.5700e-01],\n",
       "           ...,\n",
       "           [ 2.8144e-02,  2.0385e-01, -1.2203e-01,  ...,  7.4711e-03,\n",
       "             2.4381e-02,  1.6448e-01],\n",
       "           [-1.3493e-01,  7.5547e-02, -5.6866e-03,  ..., -1.9580e-02,\n",
       "            -5.9942e-02,  2.5650e-02],\n",
       "           [ 7.2733e-02,  2.4791e-01,  2.1365e-01,  ..., -2.9166e-01,\n",
       "            -1.7886e-01,  5.6788e-02]],\n",
       " \n",
       "          [[-3.9917e-01, -5.5537e-01,  3.9121e-01,  ...,  6.4231e-03,\n",
       "            -7.2457e-02, -1.2468e+00],\n",
       "           [ 3.0526e-01,  3.6878e-01, -1.3122e-01,  ...,  2.0159e-01,\n",
       "             9.3846e-02, -1.0260e+00],\n",
       "           [-1.4060e-01, -3.7322e-01,  3.4110e-01,  ..., -7.1094e-01,\n",
       "            -1.1940e-01,  8.2347e-02],\n",
       "           ...,\n",
       "           [-3.5284e-01,  1.4309e-01, -7.6366e-01,  ..., -3.9451e-01,\n",
       "            -3.0232e-01,  1.2613e-01],\n",
       "           [-1.0193e+00, -1.5043e-01, -4.0874e-01,  ..., -3.4640e-01,\n",
       "            -7.5353e-01, -3.8197e-01],\n",
       "           [-4.1050e-01,  3.3562e-01,  2.4601e-01,  ..., -1.5352e+00,\n",
       "            -1.0157e+00, -2.9939e-01]],\n",
       " \n",
       "          [[-4.7330e-01, -3.6933e-01,  3.6953e-01,  ..., -3.4727e-01,\n",
       "            -2.1887e-01, -8.4905e-01],\n",
       "           [-7.1016e-04,  1.3697e-01, -1.6524e-01,  ..., -1.8971e-01,\n",
       "             3.6336e-01, -4.3196e-01],\n",
       "           [-1.6991e-01, -2.4063e-01,  1.9735e-01,  ..., -4.9901e-01,\n",
       "            -9.6128e-02, -1.3599e-01],\n",
       "           ...,\n",
       "           [-3.9526e-01,  9.9585e-02, -8.3710e-01,  ..., -4.7766e-01,\n",
       "            -4.9449e-01, -1.4778e-01],\n",
       "           [-5.5564e-01, -3.6121e-01, -5.4792e-01,  ..., -7.3425e-01,\n",
       "            -4.2294e-01, -3.7026e-01],\n",
       "           [-3.6979e-03,  1.2487e-01,  1.8110e-02,  ..., -8.9058e-01,\n",
       "            -8.6430e-01, -2.7489e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.6644e-01, -1.2109e-02, -8.6369e-01,  ..., -5.4297e-01,\n",
       "            -4.6040e-01,  6.1359e-01],\n",
       "           [-8.0489e-01, -8.5665e-01, -4.0146e-01,  ..., -7.1823e-01,\n",
       "            -5.8236e-01,  4.2992e-01],\n",
       "           [-3.9285e-01, -1.7680e-01, -8.2674e-01,  ...,  1.2734e-01,\n",
       "            -4.0843e-01, -6.0199e-01],\n",
       "           ...,\n",
       "           [-2.0556e-01, -6.4513e-01,  1.5661e-01,  ..., -1.7112e-01,\n",
       "            -2.5949e-01, -6.4419e-01],\n",
       "           [ 4.1601e-01, -3.9523e-01, -1.6194e-01,  ..., -2.3239e-01,\n",
       "             1.7233e-01, -1.7644e-01],\n",
       "           [-1.2701e-01, -8.2598e-01, -7.4761e-01,  ...,  8.8423e-01,\n",
       "             3.9374e-01, -2.4884e-01]],\n",
       " \n",
       "          [[ 1.7323e-01,  6.9626e-01,  5.2960e-01,  ..., -3.1621e-01,\n",
       "             1.1441e-01,  9.4959e-01],\n",
       "           [-1.2029e-01,  6.1798e-02,  3.4301e-01,  ..., -3.3912e-01,\n",
       "             1.0776e+00,  1.4476e+00],\n",
       "           [ 3.5050e-01,  6.3464e-01,  2.4651e-01,  ...,  7.0556e-01,\n",
       "             4.7231e-01,  3.8902e-03],\n",
       "           ...,\n",
       "           [ 2.5693e-01,  4.0598e-01,  6.3431e-02,  ...,  1.5486e-01,\n",
       "            -5.6151e-02, -1.0466e-01],\n",
       "           [ 1.1637e+00, -5.0589e-02,  2.7610e-02,  ..., -4.9886e-01,\n",
       "             9.5285e-01,  3.6678e-01],\n",
       "           [ 1.2248e+00,  9.7882e-02,  3.2887e-02,  ...,  1.4030e+00,\n",
       "             4.7975e-01,  4.2005e-01]],\n",
       " \n",
       "          [[ 3.7921e-01,  5.0567e-01, -8.5669e-01,  ..., -1.2158e-01,\n",
       "            -9.0760e-02,  1.4747e+00],\n",
       "           [-6.0101e-01, -7.3078e-01, -4.9507e-02,  ..., -4.0475e-01,\n",
       "            -5.2708e-01,  1.0504e+00],\n",
       "           [-3.7153e-02,  2.4938e-01, -7.2664e-01,  ...,  7.3317e-01,\n",
       "            -9.2587e-02, -2.9704e-01],\n",
       "           ...,\n",
       "           [ 2.9429e-01, -4.6638e-01,  9.3832e-01,  ...,  3.7598e-01,\n",
       "             2.8194e-01, -3.4004e-01],\n",
       "           [ 1.0969e+00,  5.6936e-02,  4.2221e-01,  ...,  4.3479e-01,\n",
       "             7.4687e-01,  3.1544e-01],\n",
       "           [ 1.8721e-01, -6.8906e-01, -5.4406e-01,  ...,  1.8098e+00,\n",
       "             1.2273e+00,  1.8312e-01]]]], device='cuda:0',\n",
       "        grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 32, 286, 201]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        # 這裡假設 base_model 是一個有 TSMamba[0].time_mamba.forward_blocks[0] 結構的模型\n",
    "        self.first_block = base_model.dense_encoder.dense_conv_1[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_block(x)\n",
    "        return x\n",
    "# 假設你已經有一個已初始化好的 base_model\n",
    "my_model = CustomModel(base_model=model)\n",
    "\n",
    "# 做 forward 傳入輸入資料\n",
    "shape = [1,2,286,201]\n",
    "input_tensor = torch.randn(shape).to(\"cuda\")\n",
    "output = my_model(input_tensor)\n",
    "output, \\\n",
    "    output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0847, -0.0665, -0.0233,  ...,  0.0078, -0.1086,  0.0164],\n",
       "          [-0.0798,  0.0263, -0.0892,  ...,  0.1521,  0.0422, -0.2575],\n",
       "          [ 0.0147, -0.0233,  0.0099,  ...,  0.1294,  0.0259, -0.0891],\n",
       "          ...,\n",
       "          [-0.0520,  0.1250,  0.3329,  ..., -0.1316,  0.0767,  0.1543],\n",
       "          [ 0.0534,  0.5691,  0.3958,  ..., -0.3923, -0.3511,  0.0061],\n",
       "          [-0.0040,  0.1272,  0.1016,  ..., -0.3203, -0.0197, -0.3803]],\n",
       " \n",
       "         [[-0.0299,  0.0807,  0.0739,  ..., -0.1404,  0.0129, -0.2082],\n",
       "          [ 0.1873, -0.0825,  0.1137,  ...,  0.0411,  0.1657,  0.0345],\n",
       "          [ 0.3254,  0.0703, -0.2580,  ..., -0.0165, -0.0960,  0.0337],\n",
       "          ...,\n",
       "          [ 0.4094,  0.4627, -0.2452,  ...,  0.1872,  0.1634,  0.0524],\n",
       "          [ 0.2161, -0.0326,  0.1320,  ...,  0.0499, -0.1213, -0.1543],\n",
       "          [ 0.0373, -0.1468, -0.0747,  ...,  0.0610, -0.4242, -0.1850]],\n",
       " \n",
       "         [[-0.0614, -0.0033, -0.0147,  ..., -0.0262, -0.0140, -0.0195],\n",
       "          [-0.0478,  0.0734,  0.0580,  ..., -0.2761,  0.0636, -0.0546],\n",
       "          [-0.1280,  0.2982, -0.0960,  ...,  0.0434, -0.0613,  0.2158],\n",
       "          ...,\n",
       "          [-0.3461, -0.0640, -0.0322,  ..., -1.0001, -0.1338, -0.2097],\n",
       "          [ 0.0602,  0.1774, -0.0515,  ..., -0.2690,  0.0154, -0.4292],\n",
       "          [-0.0046, -0.3309, -0.1191,  ..., -0.1833, -0.1091, -0.1802]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0278,  0.0435,  0.0508,  ..., -0.0044, -0.0672, -0.0446],\n",
       "          [-0.0464,  0.1915,  0.0435,  ...,  0.0544,  0.2611,  0.0622],\n",
       "          [-0.0675,  0.7562,  0.1733,  ..., -1.1671,  0.6171, -0.7196],\n",
       "          ...,\n",
       "          [-0.0744, -0.0861, -0.0655,  ...,  0.1483, -0.4474, -0.3509],\n",
       "          [ 0.0585, -0.3266, -0.0545,  ...,  0.1177, -0.1902,  0.1936],\n",
       "          [-0.0977,  0.1537, -0.0936,  ..., -0.0315,  0.0117, -0.0796]],\n",
       " \n",
       "         [[-0.0072,  0.0030,  0.0472,  ..., -0.0224,  0.0352, -0.0438],\n",
       "          [ 0.0239, -0.1114,  0.1347,  ..., -0.0042,  0.0863, -0.0185],\n",
       "          [-0.1796, -0.0530, -0.0748,  ...,  0.1192, -0.0293, -0.0451],\n",
       "          ...,\n",
       "          [ 0.0146,  0.1030,  0.1895,  ...,  0.0572, -0.0483, -0.3805],\n",
       "          [-0.1037, -0.6842,  0.0056,  ...,  0.0977, -0.0463, -0.2710],\n",
       "          [ 0.1112,  0.0570,  0.3918,  ..., -0.1392, -0.0368,  0.1140]],\n",
       " \n",
       "         [[-0.0439,  0.0194, -0.0036,  ..., -0.0159,  0.0358, -0.0574],\n",
       "          [-0.0832, -0.0666,  0.0112,  ...,  0.0622, -0.0016, -0.0765],\n",
       "          [ 0.1323,  0.0312,  0.0086,  ..., -0.0083,  0.0891, -0.0567],\n",
       "          ...,\n",
       "          [ 0.2554,  0.3795,  0.0468,  ...,  0.0994,  0.2317,  0.3006],\n",
       "          [ 0.1040,  0.1279,  0.1351,  ...,  0.1793,  0.2524, -0.0055],\n",
       "          [ 0.3995,  0.2237,  0.1602,  ...,  0.2912,  0.3815, -0.1669]]],\n",
       "        device='cuda:0', grad_fn=<MambaInnerFnBackward>),\n",
       " torch.Size([100, 286, 32]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        # 這裡假設 base_model 是一個有 TSMamba[0].time_mamba.forward_blocks[0] 結構的模型\n",
    "        self.first_block = base_model.TSMamba[0].time_mamba.forward_blocks[0].mixer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_block(x)\n",
    "        return x\n",
    "# 假設你已經有一個已初始化好的 base_model\n",
    "my_model = CustomModel(base_model=model)\n",
    "\n",
    "# 做 forward 傳入輸入資料\n",
    "shape = [100,286,32]\n",
    "input_tensor = torch.randn(shape).to(\"cuda\")\n",
    "output = my_model(input_tensor)\n",
    "output, \\\n",
    "    output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
